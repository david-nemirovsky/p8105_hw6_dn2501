---
title: "Homework #6"
output: github_document
author: David Nemirovsky
date: 12/9/20
---

```{r setup, include = F}
library(tidyverse)
library(modelr)
library(mgcv)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 7,
  fig.asp = .6,
  out.width = "95%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
memory.limit(100000)
```

## **Problem 1**

```{r read homicide data}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest" ~ 0,
      disposition == "Closed by arrest" ~ 1
    )
  ) %>% 
  filter(
    victim_race %in% c("Black", "White"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```

Start with one city:

```{r baltimore}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex,
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, CI_lower, CI_upper) %>% 
  knitr::kable(digits = 3)
```

Try this across cities:

```{r glm all cities}
model_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())), 
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, CI_lower, CI_upper)
```

```{r OR plot}
model_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## **Problem 2**

Read and tidy "birthweight" data:

```{r birthweight df, message = F, warning = F}
birthweight_df = 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = fct_recode(as.factor(babysex), Male = "1", Female = "2"),
    frace = fct_recode(as.factor(frace), White = "1", Black = "2", Asian = "3", Puerto_Rican = "4", Other = "8", Unknown = "9"), 
    malform = fct_recode(as.factor(malform), Absent = "0", Present = "1"),
    mrace = fct_recode(as.factor(mrace), White = "1", Black = "2", Asian = "3", Puerto_Rican = "4", Other = "8")
  )
```

First, fit a model using mother's height and weight gain during pregnancy as predictors for birthweight:

```{r my model}
my_model = lm(bwt ~ mheight + wtgain, data = birthweight_df)

summary(my_model)
```

Then, see how its residuals look against the aforementioned covariates:

```{r resid plots}
mheight_resid =   
  birthweight_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = mheight, y = resid)) + 
  geom_point()

wtgain_resid =   
  birthweight_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = wtgain, y = resid)) + 
  geom_point()

mheight_resid + wtgain_resid
```

Since the residuals look random and symmetric enough, I will assume this is a good model to use.

Now, let's look at prediction values against the two covariates:

```{r pred plots}
mheight_pred =   
  birthweight_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = mheight, y = pred)) + 
  geom_point()

wtgain_pred = 
  birthweight_df %>% 
  add_predictions(my_model) %>% 
  add_residuals(my_model) %>% 
  ggplot(aes(x = wtgain, y = pred)) + 
  geom_point()

mheight_pred + wtgain_pred
```

Both covariates look linear enough, with mother's height showing a bit less linearity at the extreme height values.

Now, let's compare my model to Jeff's other two:

```{r model comps}
crossv_mc(birthweight_df, nrow(birthweight_df)) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble), 
    my_model = map(train, ~lm(bwt ~ mheight + wtgain, data = .x)),
    jeff_mod1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
    jeff_mod2 = map(train, ~lm(bwt ~ babysex * bhead * blength, data = .x))
  ) %>% 
  mutate(
    rmse_mine = map2_dbl(my_model, test, ~rmse(model = .x, data = .y)),
    rmse_jeff1 = map2_dbl(jeff_mod1, test, ~rmse(model = .x, data = .y)),
    rmse_jeff2 = map2_dbl(jeff_mod2, test, ~rmse(model = .x, data = .y))
  ) %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

According to the above violin plots, Jeff's second model which uses head circumference, length at birth, and sex as predictors of birthweight has the lowest root mean-squared error values, therefore making it the best model out of the 3.

## **Problem 3**

